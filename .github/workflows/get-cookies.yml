name: California Business Scraper

on:
  workflow_dispatch:  # Allows manual triggering
    inputs:
      file_numbers:
        description: 'Business file numbers to search for (comma-separated or JSON array, e.g., "123,456,789" or ["123","456","789"])'
        required: false
        default: '202250419109'
      batch_number:
        description: 'Batch number (for tracking retries)'
        required: false
        default: '1'
      test_run:
        description: 'Test run (optional)'
        required: false
        default: 'false'
      force_rebuild:
        description: 'Force rebuild Docker image'
        required: false
        default: 'false'
        type: boolean
  push:
    paths:
      - 'Dockerfile'  # Rebuild image when Dockerfile changes
      - 'requirements.txt'  # Rebuild when dependencies change
  schedule:
    # Run daily at 9 AM UTC (optional - remove if you don't want scheduled runs)
    - cron: '0 9 * * *'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/california-scraper

jobs:
  # Build and push Docker image (only when needed)
  build-image:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.force_rebuild == 'true'
    permissions:
      contents: read
      packages: write
    outputs:
      image-tag: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx with aggressive caching
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: |
          image=moby/buildkit:buildx-stable-1
          network=host
        
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Build and push with maximum caching
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        cache-from: |
          type=gha
          type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache
        cache-to: |
          type=gha,mode=max
          type=registry,ref=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:cache,mode=max
        platforms: linux/amd64
        build-args: |
          BUILDKIT_INLINE_CACHE=1

  # Run the complete scraping process with batch handling
  scrape-california-business:
    runs-on: ubuntu-latest
    needs: [build-image]
    if: always() && (needs.build-image.result == 'success' || needs.build-image.result == 'skipped')
    permissions:
      contents: read
      packages: read
      actions: write  # Required to trigger new workflows
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Determine Docker image to use
      id: image
      run: |
        # Always use the latest tag for simplicity
        IMAGE_TAG="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
        echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
        echo "Using Docker image: $IMAGE_TAG"
        
    - name: Local-first Docker build (fastest approach)
      run: |
        echo "ðŸ”¨ Local-first Docker build strategy"
        
        # Enable Docker BuildKit for maximum speed
        export DOCKER_BUILDKIT=1
        export BUILDKIT_PROGRESS=plain
        
        # Always build locally with aggressive caching
        echo "ðŸ“¦ Building locally with cache optimization..."
        
        # Try to use remote image as cache source (non-blocking)
        REMOTE_IMAGE="${{ steps.image.outputs.image_tag }}"
        
        # Build with multiple cache sources for maximum hit rate
        docker build \
          --cache-from "$REMOTE_IMAGE" \
          --cache-from "california-scraper:latest" \
          --build-arg BUILDKIT_INLINE_CACHE=1 \
          --tag california-scraper:local \
          --tag california-scraper:latest \
          . 2>/dev/null || {
          echo "âš ï¸ Cache miss, building from scratch..."
          docker build --tag california-scraper:local .
        }
        
        echo "IMAGE_TAG=california-scraper:local" >> $GITHUB_ENV
        echo "âœ… Local build complete!"
        
    - name: Parse file numbers (fast)
      id: parse-files
      run: |
        FILE_NUMBERS="${{ github.event.inputs.file_numbers || '202250419109' }}"
        BATCH_NUMBER="${{ github.event.inputs.batch_number || '1' }}"
        
        # Quick count without complex parsing
        if [[ "$FILE_NUMBERS" == *"["* ]]; then
          COUNT=$(echo "$FILE_NUMBERS" | jq '. | length' 2>/dev/null || echo "1")
        else
          COUNT=$(echo "$FILE_NUMBERS" | tr ',' '\n' | wc -l)
        fi
        
        echo "file_count=$COUNT" >> $GITHUB_OUTPUT
        echo "batch_number=$BATCH_NUMBER" >> $GITHUB_OUTPUT
        echo "Processing $COUNT files in batch #$BATCH_NUMBER"
        
    - name: Run scraper (instant start)
      id: scraper
      env:
        SOLVECAPTCHA_API_KEY: ${{ secrets.SOLVECAPTCHA_API_KEY }}
        FILE_NUMBERS: ${{ github.event.inputs.file_numbers || '202250419109' }}
        BATCH_NUMBER: ${{ steps.parse-files.outputs.batch_number }}
      run: |
        echo "ðŸš€ Starting scraper immediately..."
        
        # Run with minimal overhead
        docker run --rm \
          --name scraper-${{ github.run_number }} \
          -e SOLVECAPTCHA_API_KEY="${SOLVECAPTCHA_API_KEY}" \
          -e FILE_NUMBERS="${FILE_NUMBERS}" \
          -e BATCH_NUMBER="${BATCH_NUMBER}" \
          -e PYTHONUNBUFFERED=1 \
          -v "$(pwd):/workspace" \
          -w /workspace \
          --memory="2g" \
          --cpus="1.5" \
          "$IMAGE_TAG" \
          timeout 600s python solve_captcha_get_cookies.py
        
                 # Quick result check
         [ -f "remaining_files.json" ] && echo "blocking_detected=true" >> $GITHUB_OUTPUT || echo "blocking_detected=false" >> $GITHUB_OUTPUT
        
    - name: Trigger retry workflow (if blocked)
      if: steps.scraper.outputs.blocking_detected == 'true'
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          const data = JSON.parse(fs.readFileSync('remaining_files.json', 'utf8'));
          console.log(`ðŸ”„ Triggering retry for ${data.file_numbers.length} files`);
          
          await github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'get-cookies.yml',
            ref: 'main',
            inputs: {
              file_numbers: JSON.stringify(data.file_numbers),
              batch_number: data.batch_number.toString()
            }
          });
        
    - name: List generated files
      run: |
        echo "Files in workspace:"
        ls -la
        echo "Looking for JSON files:"
        find . -name "*.json" -type f
        
    - name: Upload scraped data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: scraped-business-data-batch-${{ steps.parse-files.outputs.batch_number }}-${{ steps.parse-files.outputs.file_count }}-files-${{ github.run_number }}
        path: |
          scraped_data_*.json
          remaining_files.json
        retention-days: 30
        if-no-files-found: warn
        
    - name: Show scraping summary
      run: |
        echo "=== BATCH ${{ steps.parse-files.outputs.batch_number }} SCRAPING SUMMARY ==="
        if [ -f scraped_data_*.json ]; then
          for file in scraped_data_*.json; do
            if [ -f "$file" ]; then
              echo "Scraped data file: $file"
              echo "File size: $(wc -c < "$file") bytes"
              
              # Try to extract summary from the JSON structure
              if command -v jq >/dev/null 2>&1; then
                echo "Metadata:"
                jq -r '.metadata // empty' "$file" 2>/dev/null || echo "  No metadata found"
                
                echo "Results summary:"
                jq -r '.results | to_entries | map("  File \(.key): \(.value.businesses_found // 0) businesses (\(.value.success // false))") | .[]' "$file" 2>/dev/null || echo "  Unable to parse results"
                
                TOTAL_BUSINESSES=$(jq '[.results[] | .businesses_found // 0] | add // 0' "$file" 2>/dev/null || echo "0")
                SUCCESSFUL_FILES=$(jq '[.results[] | select(.success == true)] | length // 0' "$file" 2>/dev/null || echo "0")
                TOTAL_FILES=$(jq '.results | length // 0' "$file" 2>/dev/null || echo "0")
                BLOCKED_STATUS=$(jq -r '.metadata.blocked // false' "$file" 2>/dev/null || echo "false")
                
                echo "ðŸ“Š Total files processed: $TOTAL_FILES"
                echo "âœ… Successful files: $SUCCESSFUL_FILES"
                echo "ðŸ¢ Total businesses found: $TOTAL_BUSINESSES"
                echo "ðŸš« Blocking detected: $BLOCKED_STATUS"
              else
                echo "jq not available, showing file size only"
              fi
              echo "ðŸ“„ Output format: JSON"
              echo "---"
            fi
          done
        else
          echo "No scraped data files found"
        fi
        
        # Show remaining files info if exists
        if [ -f "remaining_files.json" ]; then
          echo ""
          echo "ðŸ”„ REMAINING FILES INFO:"
          if command -v jq >/dev/null 2>&1; then
            REMAINING_COUNT=$(jq '.file_numbers | length' "remaining_files.json" 2>/dev/null || echo "unknown")
            NEXT_BATCH=$(jq -r '.batch_number' "remaining_files.json" 2>/dev/null || echo "unknown")
            echo "ðŸ“Š Files remaining: $REMAINING_COUNT"
            echo "ðŸ“¦ Next batch number: $NEXT_BATCH"
            echo "ðŸ”„ New workflow will be triggered automatically"
          fi
        fi
        echo "==========================" 
