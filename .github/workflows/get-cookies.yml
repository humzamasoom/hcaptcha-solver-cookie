name: California Business Scraper

on:
  workflow_dispatch:  # Allows manual triggering
    inputs:
      file_numbers:
        description: 'Business file numbers to search for (comma-separated or JSON array, e.g., "123,456,789" or ["123","456","789"])'
        required: false
        default: '202250419109'
      test_run:
        description: 'Test run (optional)'
        required: false
        default: 'false'
      force_rebuild:
        description: 'Force rebuild Docker image'
        required: false
        default: 'false'
        type: boolean
  push:
    paths:
      - 'Dockerfile'  # Rebuild image when Dockerfile changes
      - 'requirements.txt'  # Rebuild when dependencies change
  schedule:
    # Run daily at 9 AM UTC (optional - remove if you don't want scheduled runs)
    - cron: '0 9 * * *'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/california-scraper

jobs:
  # Build and push Docker image (only when needed)
  build-image:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.force_rebuild == 'true'
    permissions:
      contents: read
      packages: write
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          type=raw,value=latest,enable={{is_default_branch}}
          
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        
    - name: Image build summary
      run: |
        echo "‚úÖ Docker image built and pushed successfully!"
        echo "üì¶ Image: ${{ steps.meta.outputs.tags }}"
        echo "üîç Digest: ${{ steps.build.outputs.digest }}"

  # Run the complete scraping process
  scrape-california-business:
    runs-on: ubuntu-latest
    needs: [build-image]
    if: always() && (needs.build-image.result == 'success' || needs.build-image.result == 'skipped')
    permissions:
      contents: read
      packages: read
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Determine Docker image to use
      id: image
      run: |
        if [ "${{ needs.build-image.result }}" == "success" ]; then
          # Use the freshly built image
          IMAGE_TAG="${{ needs.build-image.outputs.image-tag }}"
        else
          # Use the latest pre-built image
          IMAGE_TAG="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
        fi
        echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
        echo "Using Docker image: $IMAGE_TAG"
        
    - name: Pull Docker image (with fallback to build)
      run: |
        IMAGE_TAG="${{ steps.image.outputs.image_tag }}"
        echo "Attempting to pull image: $IMAGE_TAG"
        
        if docker pull "$IMAGE_TAG" 2>/dev/null; then
          echo "‚úÖ Successfully pulled pre-built image: $IMAGE_TAG"
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
        else
          echo "‚ö†Ô∏è Failed to pull image, building locally as fallback..."
          docker build -t california-scraper:local .
          echo "‚úÖ Local image built successfully"
          echo "IMAGE_TAG=california-scraper:local" >> $GITHUB_ENV
        fi
        
    - name: Verify Docker image
      run: |
        echo "Verifying image: $IMAGE_TAG"
        docker images
        echo "Verifying environment in container..."
        docker run --rm "$IMAGE_TAG" python --version
        docker run --rm "$IMAGE_TAG" google-chrome --version
        
    - name: Parse and validate file numbers
      id: parse-files
      run: |
        FILE_NUMBERS="${{ github.event.inputs.file_numbers || '202250419109' }}"
        echo "Raw input: $FILE_NUMBERS"
        
        # Count how many file numbers we have
        if [[ "$FILE_NUMBERS" == *"["* ]]; then
          # JSON array format
          COUNT=$(echo "$FILE_NUMBERS" | jq '. | length' 2>/dev/null || echo "1")
        else
          # Comma-separated format
          COUNT=$(echo "$FILE_NUMBERS" | tr ',' '\n' | wc -l)
        fi
        
        echo "file_count=$COUNT" >> $GITHUB_OUTPUT
        echo "Detected $COUNT file numbers to process"
        
    - name: Run California Business Scraper
      env:
        SOLVECAPTCHA_API_KEY: ${{ secrets.SOLVECAPTCHA_API_KEY }}
        # File numbers to search (can be overridden via workflow input)
        FILE_NUMBERS: ${{ github.event.inputs.file_numbers || '202250419109' }}
      run: |
        echo "Starting California business scraper..."
        echo "Using Docker image: $IMAGE_TAG"
        echo "File numbers: $FILE_NUMBERS"
        echo "Expected file count: ${{ steps.parse-files.outputs.file_count }}"
        echo "Output format: JSON"
        
        # Run the scraper in Docker container
        docker run --rm \
          -e SOLVECAPTCHA_API_KEY="${SOLVECAPTCHA_API_KEY}" \
          -e FILE_NUMBERS="${FILE_NUMBERS}" \
          -v "$(pwd):/workspace" \
          -w /workspace \
          "$IMAGE_TAG" \
          python solve_captcha_get_cookies.py
        
    - name: List generated files
      run: |
        echo "Files in workspace:"
        ls -la
        echo "Looking for JSON files:"
        find . -name "*.json" -type f
        
    - name: Upload scraped data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: scraped-business-data-${{ steps.parse-files.outputs.file_count }}-files-${{ github.run_number }}
        path: |
          scraped_data_*.json
        retention-days: 30
        if-no-files-found: warn
        
    - name: Show scraping summary
      run: |
        echo "=== SCRAPING SUMMARY ==="
        if [ -f scraped_data_*.json ]; then
          for file in scraped_data_*.json; do
            if [ -f "$file" ]; then
              echo "Scraped data file: $file"
              echo "File size: $(wc -c < "$file") bytes"
              
              # Try to extract summary from the JSON structure
              if command -v jq >/dev/null 2>&1; then
                echo "Metadata:"
                jq -r '.metadata // empty' "$file" 2>/dev/null || echo "  No metadata found"
                
                echo "Results summary:"
                jq -r '.results | to_entries | map("  File \(.key): \(.value.businesses_found // 0) businesses (\(.value.success // false))") | .[]' "$file" 2>/dev/null || echo "  Unable to parse results"
                
                TOTAL_BUSINESSES=$(jq '[.results[] | .businesses_found // 0] | add // 0' "$file" 2>/dev/null || echo "0")
                SUCCESSFUL_FILES=$(jq '[.results[] | select(.success == true)] | length // 0' "$file" 2>/dev/null || echo "0")
                TOTAL_FILES=$(jq '.results | length // 0' "$file" 2>/dev/null || echo "0")
                
                echo "üìä Total files processed: $TOTAL_FILES"
                echo "‚úÖ Successful files: $SUCCESSFUL_FILES"
                echo "üè¢ Total businesses found: $TOTAL_BUSINESSES"
              else
                echo "jq not available, showing file size only"
              fi
              echo "üìÑ Output format: JSON"
              echo "---"
            fi
          done
        else
          echo "No scraped data files found"
        fi
        echo "========================"
        
    # Optional: Commit scraped data back to repository (uncomment if desired)
    # - name: Commit scraped data to repository
    #   run: |
    #     git config --local user.email "action@github.com"
    #     git config --local user.name "GitHub Action"
    #     git add scraped_data_*.json
    #     git commit -m "Add scraped business data for ${{ steps.parse-files.outputs.file_count }} file numbers" || exit 0
    #     git push 
