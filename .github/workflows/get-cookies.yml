name: California Business Scraper

on:
  workflow_dispatch:  # Allows manual triggering
    inputs:
      file_numbers:
        description: 'Business file numbers to search for (comma-separated or JSON array, e.g., "123,456,789" or ["123","456","789"])'
        required: false
        default: '202250419109'
      batch_number:
        description: 'Batch number (for tracking retries)'
        required: false
        default: '1'
      test_run:
        description: 'Test run (optional)'
        required: false
        default: 'false'
      force_rebuild:
        description: 'Force rebuild Docker image'
        required: false
        default: 'false'
        type: boolean
  push:
    paths:
      - 'Dockerfile'  # Rebuild image when Dockerfile changes
      - 'requirements.txt'  # Rebuild when dependencies change
  schedule:
    # Run daily at 9 AM UTC (optional - remove if you don't want scheduled runs)
    - cron: '0 9 * * *'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/california-scraper

jobs:
  # Build and push Docker image (only when needed)
  build-image:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.force_rebuild == 'true'
    permissions:
      contents: read
      packages: write
    outputs:
      image-tag: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx (for advanced caching)
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: image=moby/buildkit:buildx-stable-1
        
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=raw,value=latest,enable={{is_default_branch}}
          type=ref,event=branch
          type=sha,prefix={{branch}}-
          
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64
        
    - name: Image build summary
      run: |
        echo "‚úÖ Docker image built and pushed successfully!"
        echo "üì¶ Image: ${{ steps.meta.outputs.tags }}"
        echo "üîç Digest: ${{ steps.build.outputs.digest }}"

  # Run the complete scraping process with batch handling
  scrape-california-business:
    runs-on: ubuntu-latest
    needs: [build-image]
    if: always() && (needs.build-image.result == 'success' || needs.build-image.result == 'skipped')
    permissions:
      contents: read
      packages: read
      actions: write  # Required to trigger new workflows
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      

    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Determine Docker image to use
      id: image
      run: |
        # Always use the latest tag for simplicity
        IMAGE_TAG="${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest"
        echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
        echo "Using Docker image: $IMAGE_TAG"
        
    - name: Pull Docker image (with fallback to build)
      run: |
        IMAGE_TAG="${{ steps.image.outputs.image_tag }}"
        echo "Attempting to pull image: $IMAGE_TAG"
        
        if docker pull "$IMAGE_TAG" 2>/dev/null; then
          echo "‚úÖ Successfully pulled pre-built image: $IMAGE_TAG"
          echo "IMAGE_TAG=$IMAGE_TAG" >> $GITHUB_ENV
        else
          echo "‚ö†Ô∏è Failed to pull image, building locally as fallback..."
          docker build -t california-scraper:local .
          echo "‚úÖ Local image built successfully"
          echo "IMAGE_TAG=california-scraper:local" >> $GITHUB_ENV
        fi
        
    - name: Verify Docker image (quick check)
      run: |
        echo "Verifying image: $IMAGE_TAG"
        # Quick verification - just check if Chrome is available
        if docker run --rm "$IMAGE_TAG" which google-chrome >/dev/null; then
          echo "‚úÖ Chrome verified in container"
        else
          echo "‚ùå Chrome not found in container"
          exit 1
        fi
        
    - name: Parse and validate file numbers
      id: parse-files
      run: |
        FILE_NUMBERS="${{ github.event.inputs.file_numbers || '202250419109' }}"
        BATCH_NUMBER="${{ github.event.inputs.batch_number || '1' }}"
        echo "Raw input: $FILE_NUMBERS"
        echo "Batch number: $BATCH_NUMBER"
        
        # Count how many file numbers we have
        if [[ "$FILE_NUMBERS" == *"["* ]]; then
          # JSON array format
          COUNT=$(echo "$FILE_NUMBERS" | jq '. | length' 2>/dev/null || echo "1")
        else
          # Comma-separated format
          COUNT=$(echo "$FILE_NUMBERS" | tr ',' '\n' | wc -l)
        fi
        
        echo "file_count=$COUNT" >> $GITHUB_OUTPUT
        echo "batch_number=$BATCH_NUMBER" >> $GITHUB_OUTPUT
        echo "Detected $COUNT file numbers to process in batch #$BATCH_NUMBER"
        
    - name: Run California Business Scraper (Batch Processing)
      id: scraper
      env:
        SOLVECAPTCHA_API_KEY: ${{ secrets.SOLVECAPTCHA_API_KEY }}
        FILE_NUMBERS: ${{ github.event.inputs.file_numbers || '202250419109' }}
        BATCH_NUMBER: ${{ steps.parse-files.outputs.batch_number }}
      run: |
        echo "Starting California business scraper (Batch Processing)..."
        echo "Using Docker image: $IMAGE_TAG"
        echo "File numbers: $FILE_NUMBERS"
        echo "Expected file count: ${{ steps.parse-files.outputs.file_count }}"
        echo "Batch number: $BATCH_NUMBER"
        echo "Output format: JSON"
        
        # Run the scraper in Docker container with optimizations
        docker run --rm \
          -e SOLVECAPTCHA_API_KEY="${SOLVECAPTCHA_API_KEY}" \
          -e FILE_NUMBERS="${FILE_NUMBERS}" \
          -e BATCH_NUMBER="${BATCH_NUMBER}" \
          -e PYTHONUNBUFFERED=1 \
          -v "$(pwd):/workspace" \
          -w /workspace \
          --memory="3g" \
          --cpus="2.0" \
          --shm-size="1g" \
          "$IMAGE_TAG" \
          timeout 900s python solve_captcha_get_cookies.py
        
        # Check if there are remaining files to process
        if [ -f "remaining_files.json" ]; then
          echo "blocking_detected=true" >> $GITHUB_OUTPUT
          echo "üö´ Blocking detected - remaining files found"
        else
          echo "blocking_detected=false" >> $GITHUB_OUTPUT
          echo "‚úÖ All files processed successfully"
        fi
        
    - name: Handle remaining files (if blocked)
      if: steps.scraper.outputs.blocking_detected == 'true'
      uses: actions/github-script@v7
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          
          // Read remaining files
          const remainingData = JSON.parse(fs.readFileSync('remaining_files.json', 'utf8'));
          const remainingFiles = remainingData.file_numbers;
          const nextBatchNumber = remainingData.batch_number;
          
          console.log(`üîÑ Triggering new workflow for ${remainingFiles.length} remaining files`);
          console.log(`üìä Next batch number: ${nextBatchNumber}`);
          console.log(`üìù Remaining files: ${remainingFiles.join(', ')}`);
          
          // Trigger new workflow with remaining files
          await github.rest.actions.createWorkflowDispatch({
            owner: context.repo.owner,
            repo: context.repo.repo,
            workflow_id: 'get-cookies.yml',
            ref: 'main',
            inputs: {
              file_numbers: JSON.stringify(remainingFiles),
              batch_number: nextBatchNumber.toString(),
              test_run: 'false'
            }
          });
          
          console.log('‚úÖ New workflow triggered successfully');
        
    - name: List generated files
      run: |
        echo "Files in workspace:"
        ls -la
        echo "Looking for JSON files:"
        find . -name "*.json" -type f
        
    - name: Upload scraped data as artifact
      uses: actions/upload-artifact@v4
      with:
        name: scraped-business-data-batch-${{ steps.parse-files.outputs.batch_number }}-${{ steps.parse-files.outputs.file_count }}-files-${{ github.run_number }}
        path: |
          scraped_data_*.json
          remaining_files.json
        retention-days: 30
        if-no-files-found: warn
        
    - name: Show scraping summary
      run: |
        echo "=== BATCH ${{ steps.parse-files.outputs.batch_number }} SCRAPING SUMMARY ==="
        if [ -f scraped_data_*.json ]; then
          for file in scraped_data_*.json; do
            if [ -f "$file" ]; then
              echo "Scraped data file: $file"
              echo "File size: $(wc -c < "$file") bytes"
              
              # Try to extract summary from the JSON structure
              if command -v jq >/dev/null 2>&1; then
                echo "Metadata:"
                jq -r '.metadata // empty' "$file" 2>/dev/null || echo "  No metadata found"
                
                echo "Results summary:"
                jq -r '.results | to_entries | map("  File \(.key): \(.value.businesses_found // 0) businesses (\(.value.success // false))") | .[]' "$file" 2>/dev/null || echo "  Unable to parse results"
                
                TOTAL_BUSINESSES=$(jq '[.results[] | .businesses_found // 0] | add // 0' "$file" 2>/dev/null || echo "0")
                SUCCESSFUL_FILES=$(jq '[.results[] | select(.success == true)] | length // 0' "$file" 2>/dev/null || echo "0")
                TOTAL_FILES=$(jq '.results | length // 0' "$file" 2>/dev/null || echo "0")
                BLOCKED_STATUS=$(jq -r '.metadata.blocked // false' "$file" 2>/dev/null || echo "false")
                
                echo "üìä Total files processed: $TOTAL_FILES"
                echo "‚úÖ Successful files: $SUCCESSFUL_FILES"
                echo "üè¢ Total businesses found: $TOTAL_BUSINESSES"
                echo "üö´ Blocking detected: $BLOCKED_STATUS"
              else
                echo "jq not available, showing file size only"
              fi
              echo "üìÑ Output format: JSON"
              echo "---"
            fi
          done
        else
          echo "No scraped data files found"
        fi
        
        # Show remaining files info if exists
        if [ -f "remaining_files.json" ]; then
          echo ""
          echo "üîÑ REMAINING FILES INFO:"
          if command -v jq >/dev/null 2>&1; then
            REMAINING_COUNT=$(jq '.file_numbers | length' "remaining_files.json" 2>/dev/null || echo "unknown")
            NEXT_BATCH=$(jq -r '.batch_number' "remaining_files.json" 2>/dev/null || echo "unknown")
            echo "üìä Files remaining: $REMAINING_COUNT"
            echo "üì¶ Next batch number: $NEXT_BATCH"
            echo "üîÑ New workflow will be triggered automatically"
          fi
        fi
        echo "==========================" 
